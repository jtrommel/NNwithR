\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Neural Networks with R}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=NNR}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(tidyverse)
library(neuralnet)
library(devtools)
library(NeuralNetTools)
library(nnet)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 9 , face = "bold"),
                  axis.title = element_text(size = 10 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 9 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 9),
                  legend.title = element_text(size = 10 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions

\mainmatter

\chapter{Neural Networks with R}
\sidenote{''Neural Networks with R"; Giuseppe Ciaburro, Balaji Venkatswaran; Packt>, 2017}
\section{Neural Networks and Artificial Intelligence Concepts}
\subsection{Training a neural network to square a number}
\newthought{Reading the data}
<<>>=
# Reading the data
mydata <- read.csv("Data/squares.csv", sep=";", header=TRUE)
mydata
attach(mydata)
names(mydata)
@

\newthought{Constructing the model}

Because the model starts with a random choice of the weights and the biases, we will get another solution each time we use ''neuralnet" to determine the model parameters. Therefor we need to work with a seed for the random functions.
<<label=model,fig=TRUE, include=FALSE, echo=FALSE>>=
# Train the model based on output from input
set.seed(2018) 
model <- neuralnet(formula=OUTPUT~INPUT,
                   data=mydata,
                   hidden=10,
                   threshold=0.01)
print(model)
# Load a better plotting function
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(model)
@

\begin{figure*}
\includegraphics[width=0.8\textwidth]{NNR-model}
\caption{Plot of neural network model}
\label{fig:model}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure*}

The modelparameters are 
\begin{itemize}
  \item $B_{1,i}$: the biases for each hidden layer
  \item $win_{i}$: the weights for the inputs
  \item $whid_{i}$: the weights for the hidden layers
  \item $B_{2}$: the bias for the output
\end{itemize}

<<>>=
resultaat <- data.frame(win=model$result.matrix[seq(5,23,by=2)], B1=model$result.matrix[seq(4, 22, by=2)], whid=model$result.matrix[25:34])
B2 <- model$result.matrix[24]
resultaat
B2
@

\newthought{Comparing actual output and predicted output}

<<>>=
final_output <- cbind(INPUT, OUTPUT, as.data.frame(model$net.result))
colnames(final_output) <- c("Input","Expected Output","Neural Net Output")
print(final_output)
@

\newthought{Using the model for prediction}

<<>>=
test <- data.frame(x=seq(1.5, 9.5, by=1), xsq=seq(1.5, 9.5, by=1)^2, xmod=compute(model, seq(1.5, 9.5, by=1))$net.result)
test$error <- test$xsq - test$xmod
test
@

<<label=test,fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data=test, aes(x=x)) + 
  geom_line(aes(y=xsq)) + 
  geom_point(aes(y=xmod), color="red") + 
  JT.theme
@

\begin{marginfigure}[-4cm]
\includegraphics[width=1\textwidth]{NNR-test}
\caption{Using the neural network model}
\label{fig:test}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

\newpage
\subsection{Creating a model with 3 continuous inputs and one binary output}

\newthought{Reading the data}
<<>>=
mydata <- read.csv("Data/tip.csv", sep=";", header=TRUE)
attach(mydata)
names(mydata)
@

\newthought{Train the model using nnet}
<<>>=
model <- nnet(CustomerWillTip ~ Service + Ambience + Food,
              data=mydata,
              size=5,
              rang=0.1,
              decay=0.05,
              maxit=5000)
print(model)
garson(model)
@

\newthought{Plot the model}
<<label=model_tip,fig=TRUE, include=FALSE, echo=FALSE>>=
plotnet(model)
@

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{NNR-model_tip}
\caption{Layout of model Tip}
\label{fig:model_tip}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

\newthought{Relative importance of the input variables}
<<label=garson_tip,fig=TRUE, include=FALSE, echo=FALSE>>=
garson(model)
@

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{NNR-garson_tip}
\caption{Relative importance of input variables}
\label{fig:garson_tip}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

\section{Learning processes in Neural Networks}

Learning =

\begin{itemize}
  \item training phase: fitting the model parameters
  \item testing phase: assess the performance of the trained neural network
  \item deployment phase: actual data is given as input and the model makes predictions
\end{itemize}

\subsection{What is machine learning?}
The model is human learning: first aided training how to do a task, than doing it on your own. The learning process is never over, because we adapt our model based on new inputs. Machine learning is not the same as \emph{programming}. Programming means giving a machine a set of instructions. In machine learning the only input is data, and the model will adapt to it. The algorithm that we use is based on the characteristics of the data: the features of the independent variables, the type of the dependent variable(s), de accuracy of the model and eht speed of training the model and predicting with the model.

The features of the independent variables will determine how we will train the model:
\begin{enumerate}
  \item Supervised learning: there is a target e.g. an error term that has to be minimised
  \item Unsupervised learning: there is no specific target. The algorithm needs to find something using the data
  \item Reinforcement learning: there is constant feedback in the learning process and the algorithm adapts based on the environment
\end{enumerate}

\subsection{Supervised learning}
\emph{Supervised learning} is a method where part of the training data acts as a teacher to the algorithm to determine the model. The machine is taught what to learn form the target data. The target data are known in advance. For neural networks the values of the independent variables and the response variables must be numeric. If not a conversion is needed. Once the model is constructed based on the training data, it can be used to predict outcomes from new input data.

\subsection{Unsupervised learning}
\emph{Unsupervised learning} is also called \emph{Self Organizing Map (SOM)}: the input data are organised into another set fo data without the need of a target variable. It tries to find patterns in the input data and group them into clusters. When the model is given new input data it will reorganise these data into these different groups/clusters. Typical techniques used in unsupervised learning are:
\begin{itemize}
  \item clustering (K-means, hierarchical)
  \item association techniques
  \item dimensionality reduction
  \item Kohonen networks
\end{itemize}

Neural networks can do supervised and unsupervised learning.

\subsection{Reinforcement learning}
\emph{Reinforcement learning} requires a constant feedback system. It can be done with a special type of neural network (Q-learning), but this falls outside the scope of this book.

\subsection{Training and Testing a model}
We split the data set into a \emph{training set} (e.g. 70\% of the data) and a \emph{testing set} (e.g. 30\% of the data set). The allocation is done radomly to reduce bias. 

The training set is fed into the neural network that will determine the weights, biases and activation function to be used the obtain the output. The target is to minimise the error between the calculated output and the, known, output data.

When sufficient convergence is achieved, we feed the test data to the model and we compare the predicted outcomes with the, known, outcomes of the test data. This will lead to a \emph{confusion matrix} that we can use to evaluate the quality of the model.

Deployment means that we use the model to predict the outcome based on new input data, of which the outcome is not known.
\newpage
\textbf{Thanks} \\
\medskip
R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
\medskip
<<>>=
sessionInfo()
@
\end{document}